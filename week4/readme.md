ğŸ“˜ Overview

This project demonstrates my understanding of NumPy â€” one of the most powerful Python libraries for numerical and scientific computing.

After learning core NumPy concepts such as arrays, indexing, slicing, data manipulation, and sorting, I applied that knowledge to a real-world dataset from Kaggle.

The goal of this project was to:

Load an unclean CSV dataset

Clean invalid or missing data

Sort and organize the dataset using NumPy

Save the final cleaned version for further analysis

ğŸ§© Project Structure
numpy project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ sample_sales.csv          # Raw dataset (downloaded from Kaggle)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ clean_data_numpy.py       # NumPy script for cleaning & sorting
â”œâ”€â”€ output/
â”‚   â””â”€â”€ cleaned_sales.csv         # Cleaned and sorted data output
â””â”€â”€ README.md                     # Project documentation

ğŸ§  NumPy Concepts Used

Array creation and manipulation

Data loading with np.genfromtxt()

Handling missing / invalid data

Logical masking and filtering

Sorting arrays by specific fields

Saving cleaned data using np.savetxt()

Basic statistical operations (optional section)

âš™ï¸ Steps Performed

Loaded raw CSV data using NumPy.

Detected and replaced invalid or missing numeric values.

Removed rows with 0 or missing entries in important columns.

Sorted the dataset by SALES in descending order.

Saved the cleaned dataset as a new CSV file.

ğŸ“Š Output Example

The script generates a file:

output/cleaned_sales.csv


containing a clean, well-structured, and sorted dataset â€” ready for data analysis or visualization.

ğŸ§¾ Dataset Source

Dataset: Sample Sales Data â€“ Kaggle

ğŸ§‘â€ğŸ’» Skills Demonstrated

Data Cleaning with NumPy

Data Organization & File Handling

Problem Solving with Array Operations

Real-world Data Processing

ğŸ Conclusion

This project helped reinforce my understanding of NumPy fundamentals and how they apply in real data preprocessing workflows.
It showcases how Pythonâ€™s numerical capabilities can be used to clean and prepare messy datasets efficiently â€” a critical step in any data science pipeline.